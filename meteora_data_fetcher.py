import json
import logging
import os
import time
from collections import defaultdict
from typing import Dict, List

import pandas as pd
from dotenv import load_dotenv
from dune_client.client import DuneClient

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class MeteoraDataFetcher:
    def __init__(self, query_ids: List[int] = None):
        """
        åˆå§‹åŒ–Meteoraæ•°æ®è·å–å™¨

        Args:
            query_ids: DuneæŸ¥è¯¢IDåˆ—è¡¨ï¼Œå¦‚æœä¸æä¾›åˆ™ä½¿ç”¨é»˜è®¤å€¼
        """
        # ä»ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥
        dune_api_key = os.getenv('DUNE_API_KEY')
        if not dune_api_key:
            raise ValueError("è¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®DUNE_API_KEY")

        self.dune = DuneClient(dune_api_key)
        self.query_ids = query_ids or [5556654]  # é»˜è®¤æŸ¥è¯¢IDï¼Œæ”¯æŒå¤šä¸ª
        self.data_dir = "meteora_data"
        self.batch_data_dir = os.path.join(self.data_dir, "batches")

        # åˆ›å»ºæ•°æ®ç›®å½•
        os.makedirs(self.data_dir, exist_ok=True)
        os.makedirs(self.batch_data_dir, exist_ok=True)

    def save_raw_dune_data(self, df: pd.DataFrame, query_result):
        """ä¿å­˜åŸå§‹Duneæ•°æ®"""
        try:
            # ä¿å­˜å¤„ç†åçš„DataFrameä¸ºCSVæ ¼å¼
            csv_file = os.path.join(self.data_dir, "raw_dune_data.csv")
            df.to_csv(csv_file, index=False, encoding='utf-8')

            # ä¿å­˜å¤„ç†åçš„DataFrameä¸ºJSONæ ¼å¼
            json_file = os.path.join(self.data_dir, "raw_dune_data.json")
            df.to_json(json_file, orient='records', ensure_ascii=False, indent=2)

            # ä¿å­˜å®Œæ•´çš„åŸå§‹å“åº”æ•°æ®
            raw_response_file = os.path.join(self.data_dir, "dune_raw_response.json")
            raw_response_data = {
                "query_id": query_result.query_id,
                "result": {
                    "rows": query_result.result.rows,
                    "metadata": query_result.result.metadata.__dict__ if hasattr(query_result.result,
                                                                                 'metadata') else {}
                }
            }
            with open(raw_response_file, 'w', encoding='utf-8') as f:
                json.dump(raw_response_data, f, indent=2, ensure_ascii=False)

            # ä¿å­˜æ•°æ®æ‘˜è¦ä¿¡æ¯
            summary = {
                "query_id": query_result.query_id,
                "total_records": len(df),
                "unique_wallets": df['evt_tx_signer'].nunique(),
                "unique_pairs": df['lbPair'].nunique(),
                "columns": list(df.columns),
                "first_few_records": df.head(5).to_dict('records'),
                "data_types": df.dtypes.astype(str).to_dict(),
                "fetch_timestamp": pd.Timestamp.now().isoformat()
            }

            summary_file = os.path.join(self.data_dir, "dune_data_summary.json")
            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(summary, f, indent=2, ensure_ascii=False)

            logger.info(f"åŸå§‹Duneæ•°æ®å·²ä¿å­˜:")
            logger.info(f"  CSVæ–‡ä»¶: {csv_file}")
            logger.info(f"  JSONæ–‡ä»¶: {json_file}")
            logger.info(f"  åŸå§‹å“åº”: {raw_response_file}")
            logger.info(f"  æ‘˜è¦æ–‡ä»¶: {summary_file}")

        except Exception as e:
            logger.warning(f"ä¿å­˜åŸå§‹Duneæ•°æ®å¤±è´¥: {str(e)}")

    def fetch_single_batch(self, query_id: int, batch_name: str = None) -> pd.DataFrame:
        """
        è·å–å•ä¸ªæ‰¹æ¬¡çš„æ•°æ®

        Args:
            query_id: æŸ¥è¯¢ID
            batch_name: æ‰¹æ¬¡åç§°ï¼Œç”¨äºä¿å­˜æ–‡ä»¶

        Returns:
            DataFrame: è¯¥æ‰¹æ¬¡çš„æ•°æ®
        """
        if not batch_name:
            batch_name = f"batch_{query_id}"

        logger.info(f"è·å–æ‰¹æ¬¡ '{batch_name}' (æŸ¥è¯¢ID: {query_id}) çš„æ•°æ®...")

        try:
            # ä½¿ç”¨get_latest_resultè·å–åŸå§‹ç»“æœ
            query_result = self.dune.get_latest_result(query_id)

            if not query_result or not query_result.result or not query_result.result.rows:
                logger.warning(f"æ‰¹æ¬¡ '{batch_name}' æœªè·å–åˆ°æ•°æ®æˆ–æ•°æ®ä¸ºç©º")
                return pd.DataFrame()

            # ä»result.rowsä¸­æå–æ•°æ®
            rows_data = query_result.result.rows
            df = pd.DataFrame(rows_data)

            if df.empty:
                logger.warning(f"æ‰¹æ¬¡ '{batch_name}' è·å–åˆ°çš„æ•°æ®ä¸ºç©º")
                return df

            # éªŒè¯å¿…è¦åˆ—
            required_columns = ['evt_tx_signer', 'lbPair']
            missing_columns = [col for col in required_columns if col not in df.columns]

            if missing_columns:
                logger.error(f"æ‰¹æ¬¡ '{batch_name}' æ•°æ®ä¸­ç¼ºå°‘å¿…è¦åˆ—: {missing_columns}")
                return pd.DataFrame()

            logger.info(f"æ‰¹æ¬¡ '{batch_name}' æˆåŠŸè·å– {len(df)} æ¡è®°å½•")

            # ä¿å­˜æ‰¹æ¬¡æ•°æ®
            self.save_batch_data(df, query_result, batch_name, query_id)

            return df

        except Exception as e:
            logger.error(f"è·å–æ‰¹æ¬¡ '{batch_name}' æ•°æ®å¤±è´¥: {str(e)}")
            return pd.DataFrame()

    def fetch_all_batches(self, delay_seconds: float = 1.0, preserve_batches: bool = True) -> List[pd.DataFrame]:
        """
        è·å–æ‰€æœ‰æ‰¹æ¬¡çš„æ•°æ®

        Args:
            delay_seconds: æ¯ä¸ªæ‰¹æ¬¡ä¹‹é—´çš„å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
            preserve_batches: æ˜¯å¦ä¿ç•™å†å²æ‰¹æ¬¡æ•°æ®ï¼ˆé¿å…è¦†ç›–ï¼‰

        Returns:
            List[DataFrame]: æ‰€æœ‰æ‰¹æ¬¡çš„æ•°æ®åˆ—è¡¨
        """
        logger.info(f"å¼€å§‹è·å– {len(self.query_ids)} ä¸ªæ‰¹æ¬¡çš„æ•°æ®...")

        batch_dataframes = []

        # ç”Ÿæˆæ—¶é—´æˆ³ç”¨äºæ‰¹æ¬¡å‘½å
        import datetime
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")

        for i, query_id in enumerate(self.query_ids):
            if preserve_batches:
                # ä½¿ç”¨æ—¶é—´æˆ³é¿å…è¦†ç›–å†å²æ•°æ®
                batch_name = f"batch_{timestamp}_{i + 1}_{query_id}"
            else:
                # ä¼ ç»Ÿå‘½åæ–¹å¼ï¼ˆä¼šè¦†ç›–ï¼‰
                batch_name = f"batch_{i + 1}_{query_id}"

            logger.info(f"è·å–æ‰¹æ¬¡: {batch_name}")

            # è·å–å•ä¸ªæ‰¹æ¬¡æ•°æ®
            df = self.fetch_single_batch(query_id, batch_name)

            if not df.empty:
                batch_dataframes.append(df)

            # æ·»åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶ï¼ˆé™¤äº†æœ€åä¸€ä¸ªï¼‰
            if i < len(self.query_ids) - 1:
                logger.info(f"ç­‰å¾… {delay_seconds} ç§’åè·å–ä¸‹ä¸€æ‰¹æ¬¡...")
                time.sleep(delay_seconds)

        logger.info(f"å®Œæˆæ‰€æœ‰æ‰¹æ¬¡æ•°æ®è·å–ï¼Œå…±è·å– {len(batch_dataframes)} ä¸ªæœ‰æ•ˆæ‰¹æ¬¡")
        return batch_dataframes

    def merge_batch_data(self, batch_dataframes: List[pd.DataFrame]) -> pd.DataFrame:
        """
        åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„æ•°æ®

        Args:
            batch_dataframes: æ‰¹æ¬¡æ•°æ®åˆ—è¡¨

        Returns:
            DataFrame: åˆå¹¶åçš„æ•°æ®
        """
        if not batch_dataframes:
            logger.warning("æ²¡æœ‰å¯åˆå¹¶çš„æ‰¹æ¬¡æ•°æ®")
            return pd.DataFrame()

        logger.info(f"å¼€å§‹åˆå¹¶ {len(batch_dataframes)} ä¸ªæ‰¹æ¬¡çš„æ•°æ®...")

        # åˆå¹¶æ‰€æœ‰DataFrame
        merged_df = pd.concat(batch_dataframes, ignore_index=True)

        # å»é‡ï¼ˆåŸºäºé’±åŒ…åœ°å€å’Œäº¤æ˜“å¯¹ï¼‰
        initial_count = len(merged_df)
        merged_df = merged_df.drop_duplicates(subset=['evt_tx_signer', 'lbPair'], keep='first')
        final_count = len(merged_df)

        logger.info(f"æ•°æ®åˆå¹¶å®Œæˆï¼š")
        logger.info(f"  åˆå¹¶å‰æ€»è®°å½•æ•°: {initial_count}")
        logger.info(f"  å»é‡åæ€»è®°å½•æ•°: {final_count}")
        logger.info(f"  å»é™¤é‡å¤è®°å½•: {initial_count - final_count}")

        return merged_df

    def save_batch_data(self, df: pd.DataFrame, query_result, batch_name: str, query_id: int):
        """
        ä¿å­˜å•ä¸ªæ‰¹æ¬¡çš„æ•°æ®

        Args:
            df: æ‰¹æ¬¡æ•°æ®DataFrame
            query_result: DuneæŸ¥è¯¢ç»“æœ
            batch_name: æ‰¹æ¬¡åç§°
            query_id: æŸ¥è¯¢ID
        """
        try:
            batch_dir = os.path.join(self.batch_data_dir, batch_name)
            os.makedirs(batch_dir, exist_ok=True)

            # ä¿å­˜CSVæ ¼å¼
            csv_file = os.path.join(batch_dir, f"{batch_name}.csv")
            df.to_csv(csv_file, index=False, encoding='utf-8')

            # ä¿å­˜JSONæ ¼å¼
            json_file = os.path.join(batch_dir, f"{batch_name}.json")
            df.to_json(json_file, orient='records', ensure_ascii=False, indent=2)

            # ä¿å­˜åŸå§‹rowsæ•°æ®ï¼ˆåªä¿å­˜rowsï¼Œä¸åŒ…å«metadataï¼‰
            raw_rows_file = os.path.join(batch_dir, f"{batch_name}_raw_rows.json")
            raw_rows_data = {
                "batch_name": batch_name,
                "query_id": query_result.query_id,
                "rows": query_result.result.rows  # åªä¿å­˜rowsæ•°æ®
            }
            with open(raw_rows_file, 'w', encoding='utf-8') as f:
                json.dump(raw_rows_data, f, indent=2, ensure_ascii=False)

            # ä¿å­˜æ‰¹æ¬¡æ‘˜è¦
            summary = {
                "batch_name": batch_name,
                "query_id": query_id,
                "total_records": len(df),
                "unique_wallets": df['evt_tx_signer'].nunique(),
                "unique_pairs": df['lbPair'].nunique(),
                "columns": list(df.columns),
                "data_types": df.dtypes.astype(str).to_dict(),
                "fetch_timestamp": pd.Timestamp.now().isoformat()
            }

            summary_file = os.path.join(batch_dir, f"{batch_name}_summary.json")
            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(summary, f, indent=2, ensure_ascii=False)

            logger.info(f"æ‰¹æ¬¡ '{batch_name}' æ•°æ®å·²ä¿å­˜åˆ°: {batch_dir}")

        except Exception as e:
            logger.warning(f"ä¿å­˜æ‰¹æ¬¡ '{batch_name}' æ•°æ®å¤±è´¥: {str(e)}")

    def save_merged_data(self, merged_df: pd.DataFrame, batch_dataframes: List[pd.DataFrame]):
        """
        ä¿å­˜åˆå¹¶åçš„å®Œæ•´æ•°æ®

        Args:
            merged_df: åˆå¹¶åçš„æ•°æ®
            batch_dataframes: åŸå§‹æ‰¹æ¬¡æ•°æ®åˆ—è¡¨
        """
        try:
            # ä¿å­˜åˆå¹¶åçš„CSVæ ¼å¼
            merged_csv_file = os.path.join(self.data_dir, "merged_dune_data.csv")
            merged_df.to_csv(merged_csv_file, index=False, encoding='utf-8')

            # ä¿å­˜åˆå¹¶åçš„JSONæ ¼å¼
            merged_json_file = os.path.join(self.data_dir, "merged_dune_data.json")
            merged_df.to_json(merged_json_file, orient='records', ensure_ascii=False, indent=2)

            # åˆ›å»ºåˆå¹¶æ‘˜è¦ä¿¡æ¯
            merge_summary = {
                "total_batches": len(batch_dataframes),
                "batch_record_counts": [len(df) for df in batch_dataframes],
                "merged_total_records": len(merged_df),
                "unique_wallets": merged_df['evt_tx_signer'].nunique(),
                "unique_pairs": merged_df['lbPair'].nunique(),
                "columns": list(merged_df.columns),
                "data_types": merged_df.dtypes.astype(str).to_dict(),
                "merge_timestamp": pd.Timestamp.now().isoformat(),
                "blockchain": "Solana",
                "project": "Meteora DLMM"
            }

            # ä¿å­˜åˆå¹¶æ‘˜è¦
            summary_file = os.path.join(self.data_dir, "merge_summary.json")
            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(merge_summary, f, indent=2, ensure_ascii=False)

            logger.info(f"åˆå¹¶æ•°æ®å·²ä¿å­˜:")
            logger.info(f"  CSVæ–‡ä»¶: {merged_csv_file}")
            logger.info(f"  JSONæ–‡ä»¶: {merged_json_file}")
            logger.info(f"  æ‘˜è¦æ–‡ä»¶: {summary_file}")

        except Exception as e:
            logger.warning(f"ä¿å­˜åˆå¹¶æ•°æ®å¤±è´¥: {str(e)}")

    def get_dune_data(self, delay_seconds: float = 1.0, preserve_batches: bool = True) -> pd.DataFrame:
        """
        ä»Duneè·å–æ‰€æœ‰æ‰¹æ¬¡æ•°æ®å¹¶åˆå¹¶

        Args:
            delay_seconds: æ¯ä¸ªæ‰¹æ¬¡ä¹‹é—´çš„å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
            preserve_batches: æ˜¯å¦ä¿ç•™å†å²æ‰¹æ¬¡æ•°æ®ï¼ˆé¿å…è¦†ç›–ï¼‰

        Returns:
            DataFrame: åˆå¹¶åçš„æ‰€æœ‰æ•°æ®
        """
        logger.info(f"å¼€å§‹ä»Duneè·å–æ•°æ®ï¼Œå…± {len(self.query_ids)} ä¸ªæŸ¥è¯¢...")

        if preserve_batches:
            logger.info("ğŸ”’ å¯ç”¨æ‰¹æ¬¡ä¿æŠ¤æ¨¡å¼ï¼Œä¸ä¼šè¦†ç›–å†å²æ•°æ®")
        else:
            logger.warning("âš ï¸  æ‰¹æ¬¡ä¿æŠ¤å·²å…³é—­ï¼Œå¯èƒ½ä¼šè¦†ç›–å†å²æ•°æ®")

        try:
            # è·å–æ‰€æœ‰æ‰¹æ¬¡æ•°æ®
            batch_dataframes = self.fetch_all_batches(delay_seconds, preserve_batches)

            if not batch_dataframes:
                raise Exception("æ‰€æœ‰æ‰¹æ¬¡éƒ½æœªè·å–åˆ°æœ‰æ•ˆæ•°æ®")

            # åˆå¹¶æ‰¹æ¬¡æ•°æ®
            merged_df = self.merge_batch_data(batch_dataframes)

            if merged_df.empty:
                raise Exception("åˆå¹¶åçš„æ•°æ®ä¸ºç©º")

            # ä¿å­˜åˆå¹¶åçš„å®Œæ•´æ•°æ®
            self.save_merged_data(merged_df, batch_dataframes)

            return merged_df

        except Exception as e:
            logger.error(f"è·å–Duneæ•°æ®å¤±è´¥: {str(e)}")
            raise

    def process_wallet_data(self, df: pd.DataFrame) -> Dict[str, List[str]]:
        """å¤„ç†é’±åŒ…æ•°æ®ï¼ŒæŒ‰é’±åŒ…åœ°å€åˆ†ç»„lbPair"""
        wallet_pairs = defaultdict(set)

        for _, row in df.iterrows():
            wallet = row['evt_tx_signer']
            lb_pair = row['lbPair']

            if pd.notna(wallet) and pd.notna(lb_pair):
                wallet_pairs[wallet].add(lb_pair)

        # è½¬æ¢ä¸ºåˆ—è¡¨
        result = {wallet: list(pairs) for wallet, pairs in wallet_pairs.items()}

        logger.info(f"å¤„ç†å®Œæˆï¼š{len(result)} ä¸ªå”¯ä¸€é’±åŒ…")
        total_pairs = sum(len(pairs) for pairs in result.values())
        logger.info(f"æ€»è®¡ {total_pairs} ä¸ªé’±åŒ…-äº¤æ˜“å¯¹ç»„åˆ")

        return result

    def create_wallet_index(self, wallet_data: Dict[str, List[str]], max_files: int = 16, max_wallets_per_file: int = 10000) -> Dict[str, str]:
        """
        åˆ›å»ºé’±åŒ…ç´¢å¼•ï¼Œç”¨äºå¿«é€ŸæŸ¥æ‰¾
        ä¼˜åŒ–çš„åˆ†ç»„ç­–ç•¥ï¼šæ§åˆ¶æ–‡ä»¶æ•°é‡ï¼Œé€‚åˆGitHubä»“åº“

        Args:
            wallet_data: é’±åŒ…æ•°æ®
            max_files: æœ€å¤§æ–‡ä»¶æ•°é‡
            max_wallets_per_file: æ¯ä¸ªæ–‡ä»¶æœ€å¤§é’±åŒ…æ•°é‡
        """
        index = {}

        # ä½¿ç”¨16è¿›åˆ¶å­—ç¬¦ä½œä¸ºç¬¬ä¸€çº§åˆ†ç»„ (0-9, a-f)
        hex_chars = '0123456789abcdef'
        primary_groups = defaultdict(dict)

        # ç¬¬ä¸€çº§åˆ†ç»„ï¼šæŒ‰é’±åŒ…åœ°å€ç¬¬ä¸€ä¸ªå­—ç¬¦åˆ†ç»„
        for wallet, pairs in wallet_data.items():
            first_char = wallet[0].lower()
            if first_char in hex_chars:
                primary_groups[first_char][wallet] = pairs
            else:
                # éæ ‡å‡†å­—ç¬¦å½’å…¥ 'other' ç»„
                primary_groups['other'][wallet] = pairs

        logger.info(f"ç¬¬ä¸€çº§åˆ†ç»„å®Œæˆï¼Œå…± {len(primary_groups)} ä¸ªç»„")

        # æ£€æŸ¥æ˜¯å¦éœ€è¦è¿›ä¸€æ­¥ç»†åˆ†
        final_groups = {}

        for group_key, group_data in primary_groups.items():
            if len(group_data) <= max_wallets_per_file:
                # ä¸éœ€è¦ç»†åˆ†
                final_groups[group_key] = group_data
                logger.info(f"ç»„ '{group_key}': {len(group_data)} ä¸ªé’±åŒ…ï¼Œæ— éœ€ç»†åˆ†")
            else:
                # éœ€è¦ç»†åˆ†ï¼šæŒ‰ç¬¬äºŒä¸ªå­—ç¬¦è¿›ä¸€æ­¥åˆ†ç»„
                logger.info(f"ç»„ '{group_key}': {len(group_data)} ä¸ªé’±åŒ…ï¼Œéœ€è¦ç»†åˆ†")
                sub_groups = defaultdict(dict)

                for wallet, pairs in group_data.items():
                    if len(wallet) > 1:
                        second_char = wallet[1].lower()
                        if second_char in hex_chars:
                            sub_key = f"{group_key}_{second_char}"
                        else:
                            sub_key = f"{group_key}_other"
                    else:
                        sub_key = f"{group_key}_short"

                    sub_groups[sub_key][wallet] = pairs

                # å°†ç»†åˆ†åçš„ç»„åŠ å…¥æœ€ç»ˆç»„
                for sub_key, sub_data in sub_groups.items():
                    final_groups[sub_key] = sub_data
                    logger.info(f"  å­ç»„ '{sub_key}': {len(sub_data)} ä¸ªé’±åŒ…")

        # åˆ›å»ºæ–‡ä»¶å¹¶å»ºç«‹ç´¢å¼•
        total_files = 0
        for group_key, group_data in final_groups.items():
            filename = f"wallets_{group_key}.json"
            filepath = os.path.join(self.data_dir, filename)

            # åˆ›å»ºä¼˜åŒ–çš„æ•°æ®ç»“æ„
            optimized_data = {
                "group_info": {
                    "group_key": group_key,
                    "wallet_count": len(group_data),
                    "total_pairs": sum(len(pairs) for pairs in group_data.values()),
                    "created_at": pd.Timestamp.now().isoformat()
                },
                "wallets": group_data
            }

            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(optimized_data, f, separators=(',', ':'), ensure_ascii=False)

            # è®°å½•æ¯ä¸ªé’±åŒ…å±äºå“ªä¸ªæ–‡ä»¶ - ç¡®ä¿æ‰€æœ‰é’±åŒ…éƒ½è¢«ç´¢å¼•
            for wallet in group_data.keys():
                index[wallet] = filename
                logger.debug(f"ç´¢å¼•é’±åŒ… {wallet} -> {filename}")

            total_files += 1

            # è®¡ç®—æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(filepath) / (1024 * 1024)  # MB
            logger.info(f"åˆ›å»ºæ–‡ä»¶ '{filename}': {len(group_data)} ä¸ªé’±åŒ…, {file_size:.2f} MB")

        # éªŒè¯ç´¢å¼•å®Œæ•´æ€§
        total_wallets_in_data = len(wallet_data)
        total_wallets_in_index = len(index)

        if total_wallets_in_data != total_wallets_in_index:
            logger.warning(f"ç´¢å¼•ä¸å®Œæ•´ï¼åŸå§‹æ•°æ®: {total_wallets_in_data} ä¸ªé’±åŒ…ï¼Œç´¢å¼•: {total_wallets_in_index} ä¸ªé’±åŒ…")

            # æ‰¾å‡ºç¼ºå¤±çš„é’±åŒ…
            missing_wallets = set(wallet_data.keys()) - set(index.keys())
            if missing_wallets:
                logger.error(f"ç¼ºå¤±çš„é’±åŒ…åœ°å€: {list(missing_wallets)[:10]}...")  # åªæ˜¾ç¤ºå‰10ä¸ª
        else:
            logger.info(f"âœ… ç´¢å¼•å®Œæ•´æ€§éªŒè¯é€šè¿‡: {total_wallets_in_index} ä¸ªé’±åŒ…")

        logger.info(f"ç´¢å¼•åˆ›å»ºå®Œæˆï¼Œå…±åˆ›å»º {total_files} ä¸ªæ–‡ä»¶")
        return index

    def save_optimized_data(self, wallet_data: Dict[str, List[str]], max_files: int = 16, max_wallets_per_file: int = 10000):
        """
        ä¿å­˜ä¼˜åŒ–åçš„æ•°æ®ç»“æ„

        Args:
            wallet_data: é’±åŒ…æ•°æ®
            max_files: æœ€å¤§æ–‡ä»¶æ•°é‡ï¼ˆç”¨äºGitHubä»“åº“ä¼˜åŒ–ï¼‰
            max_wallets_per_file: æ¯ä¸ªæ–‡ä»¶æœ€å¤§é’±åŒ…æ•°é‡
        """

        # 1. åˆ›å»ºé’±åŒ…åˆ†ç»„æ–‡ä»¶å’Œç´¢å¼•
        wallet_index = self.create_wallet_index(wallet_data, max_files, max_wallets_per_file)

        # 2. ä¿å­˜é’±åŒ…ç´¢å¼•ï¼ˆå‹ç¼©æ ¼å¼ï¼Œé€‚åˆGitHubï¼‰
        index_file = os.path.join(self.data_dir, "wallet_index.json")
        with open(index_file, 'w', encoding='utf-8') as f:
            json.dump(wallet_index, f, separators=(',', ':'), ensure_ascii=False)

        # 3. ä¿å­˜å…ƒæ•°æ®
        total_files = len(set(wallet_index.values()))
        metadata = {
            "total_wallets": len(wallet_data),
            "total_pairs": sum(len(pairs) for pairs in wallet_data.values()),
            "total_files": total_files,
            "max_files_limit": max_files,
            "max_wallets_per_file": max_wallets_per_file,
            "last_updated": pd.Timestamp.now().isoformat(),
            "data_structure": "ä¼˜åŒ–åˆ†ç»„å­˜å‚¨ï¼Œé€‚åˆGitHubä»“åº“",
            "blockchain": "Solana",
            "project": "Meteora DLMM",
            "storage_strategy": "16è¿›åˆ¶å­—ç¬¦åˆ†ç»„ï¼Œè‡ªåŠ¨ç»†åˆ†å¤§æ–‡ä»¶",
            "github_optimized": True
        }

        metadata_file = os.path.join(self.data_dir, "metadata.json")
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, separators=(',', ':'), ensure_ascii=False)

        # 4. åˆ›å»ºé’±åŒ…åˆ—è¡¨ï¼ˆå‹ç¼©æ ¼å¼ï¼Œç”¨äºå‰ç«¯æœç´¢æç¤ºï¼‰
        wallet_list = sorted(list(wallet_data.keys()))  # æ’åºä¾¿äºæœç´¢
        wallet_list_file = os.path.join(self.data_dir, "wallet_list.json")
        with open(wallet_list_file, 'w', encoding='utf-8') as f:
            json.dump(wallet_list, f, separators=(',', ':'), ensure_ascii=False)

        # 5. åˆ›å»ºæŸ¥è¯¢å¸®åŠ©æ–‡æ¡£
        query_help = {
            "how_to_query": "æ ¹æ®é’±åŒ…åœ°å€æŸ¥è¯¢å¯¹åº”çš„æ•°æ®æ–‡ä»¶",
            "steps": [
                "1. ä» wallet_index.json ä¸­æŸ¥æ‰¾é’±åŒ…åœ°å€å¯¹åº”çš„æ–‡ä»¶å",
                "2. åŠ è½½å¯¹åº”çš„ wallets_*.json æ–‡ä»¶",
                "3. ä»æ–‡ä»¶çš„ wallets å­—æ®µä¸­è·å–è¯¥é’±åŒ…çš„ lbPair åˆ—è¡¨"
            ],
            "example": {
                "wallet": "9WzDXwBbmkg8ZTbNMqUxvQRAyrZzDsGYdLVL9zYtAWWM",
                "step1": "æŸ¥æ‰¾ wallet_index.json['9WzDXwBbmkg8ZTbNMqUxvQRAyrZzDsGYdLVL9zYtAWWM']",
                "step2": "å‡è®¾è¿”å› 'wallets_9.json'ï¼Œåˆ™åŠ è½½è¯¥æ–‡ä»¶",
                "step3": "è·å– wallets_9.json['wallets']['9WzDXwBbmkg8ZTbNMqUxvQRAyrZzDsGYdLVL9zYtAWWM']"
            },
            "file_structure": "wallets_[group].json",
            "total_files": total_files
        }

        help_file = os.path.join(self.data_dir, "query_help.json")
        with open(help_file, 'w', encoding='utf-8') as f:
            json.dump(query_help, f, indent=2, ensure_ascii=False)

        logger.info("æ•°æ®ä¼˜åŒ–å­˜å‚¨å®Œæˆ")
        logger.info(f"æ•°æ®ç›®å½•: {self.data_dir}")
        logger.info(f"æ€»æ–‡ä»¶æ•°: {total_files} (é™åˆ¶: {max_files})")
        logger.info(f"ç´¢å¼•æ–‡ä»¶: {index_file}")
        logger.info(f"å…ƒæ•°æ®æ–‡ä»¶: {metadata_file}")
        logger.info(f"æŸ¥è¯¢å¸®åŠ©: {help_file}")
        logger.info("âœ… GitHubä»“åº“ä¼˜åŒ–å­˜å‚¨ç­–ç•¥å·²åº”ç”¨")

    def rebuild_wallet_index(self):
        """
        é‡å»ºé’±åŒ…ç´¢å¼•æ–‡ä»¶
        æ‰«ææ‰€æœ‰ wallets_*.json æ–‡ä»¶ï¼Œé‡æ–°ç”Ÿæˆå®Œæ•´çš„ç´¢å¼•
        """
        logger.info("ğŸ”§ å¼€å§‹é‡å»ºé’±åŒ…ç´¢å¼•...")

        index = {}
        total_wallets = 0

        # æ‰«ææ‰€æœ‰é’±åŒ…æ•°æ®æ–‡ä»¶
        import glob
        wallet_files = glob.glob(os.path.join(self.data_dir, "wallets_*.json"))

        for filepath in wallet_files:
            filename = os.path.basename(filepath)
            logger.info(f"å¤„ç†æ–‡ä»¶: {filename}")

            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    file_data = json.load(f)

                # æ£€æŸ¥æ–‡ä»¶ç»“æ„
                if 'wallets' in file_data:
                    wallets = file_data['wallets']
                else:
                    # æ—§æ ¼å¼ï¼Œç›´æ¥æ˜¯é’±åŒ…æ•°æ®
                    wallets = file_data

                # ä¸ºæ¯ä¸ªé’±åŒ…å»ºç«‹ç´¢å¼•
                for wallet in wallets.keys():
                    index[wallet] = filename
                    total_wallets += 1

                logger.info(f"  ä» {filename} ç´¢å¼•äº† {len(wallets)} ä¸ªé’±åŒ…")

            except Exception as e:
                logger.error(f"å¤„ç†æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {str(e)}")

        # ä¿å­˜é‡å»ºçš„ç´¢å¼•
        index_file = os.path.join(self.data_dir, "wallet_index.json")
        with open(index_file, 'w', encoding='utf-8') as f:
            json.dump(index, f, separators=(',', ':'), ensure_ascii=False)

        logger.info(f"âœ… ç´¢å¼•é‡å»ºå®Œæˆ!")
        logger.info(f"   å¤„ç†äº† {len(wallet_files)} ä¸ªæ•°æ®æ–‡ä»¶")
        logger.info(f"   ç´¢å¼•äº† {total_wallets} ä¸ªé’±åŒ…åœ°å€")
        logger.info(f"   ç´¢å¼•æ–‡ä»¶: {index_file}")

        return index

    def load_existing_wallet_data(self) -> Dict[str, List[str]]:
        """
        åŠ è½½ç°æœ‰çš„é’±åŒ…æ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        ç”¨äºç´¯ç§¯åˆå¹¶å¤šæ¬¡è¿è¡Œçš„æ•°æ®
        """
        backup_file = os.path.join(self.data_dir, "full_wallet_data_backup.json")

        if os.path.exists(backup_file):
            try:
                with open(backup_file, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
                logger.info(f"åŠ è½½ç°æœ‰é’±åŒ…æ•°æ®: {len(existing_data)} ä¸ªé’±åŒ…")
                return existing_data
            except Exception as e:
                logger.warning(f"åŠ è½½ç°æœ‰æ•°æ®å¤±è´¥: {str(e)}")
                return {}
        else:
            logger.info("æœªæ‰¾åˆ°ç°æœ‰æ•°æ®æ–‡ä»¶ï¼Œå°†åˆ›å»ºæ–°çš„æ•°æ®é›†")
            return {}

    def merge_wallet_data(self, existing_data: Dict[str, List[str]], new_data: Dict[str, List[str]]) -> Dict[str, List[str]]:
        """
        åˆå¹¶ç°æœ‰æ•°æ®å’Œæ–°æ•°æ®

        Args:
            existing_data: ç°æœ‰çš„é’±åŒ…æ•°æ®
            new_data: æ–°è·å–çš„é’±åŒ…æ•°æ®

        Returns:
            åˆå¹¶åçš„é’±åŒ…æ•°æ®
        """
        logger.info("å¼€å§‹åˆå¹¶é’±åŒ…æ•°æ®...")

        # ä½¿ç”¨setæ¥é¿å…é‡å¤
        merged_data = defaultdict(set)

        # æ·»åŠ ç°æœ‰æ•°æ®
        for wallet, pairs in existing_data.items():
            merged_data[wallet].update(pairs)

        # æ·»åŠ æ–°æ•°æ®
        for wallet, pairs in new_data.items():
            merged_data[wallet].update(pairs)

        # è½¬æ¢å›åˆ—è¡¨æ ¼å¼
        result = {wallet: list(pairs) for wallet, pairs in merged_data.items()}

        # ç»Ÿè®¡ä¿¡æ¯
        existing_wallets = len(existing_data)
        new_wallets = len(new_data)
        merged_wallets = len(result)

        existing_pairs = sum(len(pairs) for pairs in existing_data.values())
        new_pairs = sum(len(pairs) for pairs in new_data.values())
        merged_pairs = sum(len(pairs) for pairs in result.values())

        logger.info(f"æ•°æ®åˆå¹¶å®Œæˆ:")
        logger.info(f"  ç°æœ‰é’±åŒ…: {existing_wallets} -> æ–°é’±åŒ…: {new_wallets} -> åˆå¹¶å: {merged_wallets}")
        logger.info(f"  ç°æœ‰äº¤æ˜“å¯¹: {existing_pairs} -> æ–°äº¤æ˜“å¯¹: {new_pairs} -> åˆå¹¶å: {merged_pairs}")
        logger.info(f"  æ–°å¢é’±åŒ…: {merged_wallets - existing_wallets}")
        logger.info(f"  æ–°å¢äº¤æ˜“å¯¹: {merged_pairs - existing_pairs}")

        return result

    def create_simple_lookup_api_data(self, wallet_data: Dict[str, List[str]]):
        """
        åˆ›å»ºç®€å•çš„æŸ¥æ‰¾APIæ•°æ®ç»“æ„
        å°†æ‰€æœ‰æ•°æ®å­˜å‚¨åœ¨ä¸€ä¸ªç»è¿‡ä¼˜åŒ–çš„JSONæ–‡ä»¶ä¸­
        """
        # åˆ›å»ºå‹ç¼©çš„æ•°æ®ç»“æ„
        compressed_data = {}

        for wallet, pairs in wallet_data.items():
            # åªå­˜å‚¨å¿…è¦ä¿¡æ¯
            compressed_data[wallet] = pairs

        # ä¿å­˜å‹ç¼©æ•°æ®
        api_data_file = os.path.join(self.data_dir, "wallet_pairs_api.json")
        with open(api_data_file, 'w', encoding='utf-8') as f:
            json.dump(compressed_data, f, separators=(',', ':'), ensure_ascii=False)

        logger.info(f"APIæ•°æ®æ–‡ä»¶å·²åˆ›å»º: {api_data_file}")

        # è®¡ç®—æ–‡ä»¶å¤§å°
        file_size = os.path.getsize(api_data_file) / (1024 * 1024)  # MB
        logger.info(f"APIæ•°æ®æ–‡ä»¶å¤§å°: {file_size:.2f} MB")

        if file_size > 10:  # å¦‚æœæ–‡ä»¶å¤§äº10MBï¼Œå»ºè®®ä½¿ç”¨åˆ†ç»„å­˜å‚¨
            logger.warning("æ•°æ®æ–‡ä»¶è¾ƒå¤§ï¼Œå»ºè®®ä½¿ç”¨åˆ†ç»„å­˜å‚¨æ–¹æ¡ˆ")

    def run_data_fetch(self, use_grouped_storage: bool = True, preserve_batches: bool = True,
                      batch_delay: float = 1.0, accumulate_data: bool = True):
        """
        è¿è¡Œå®Œæ•´çš„æ•°æ®è·å–å’Œå­˜å‚¨æµç¨‹

        Args:
            use_grouped_storage: æ˜¯å¦ä½¿ç”¨åˆ†ç»„å­˜å‚¨ï¼ˆæ¨èç”¨äºå¤§æ•°æ®é‡ï¼‰
            preserve_batches: æ˜¯å¦ä¿ç•™å†å²æ‰¹æ¬¡æ•°æ®ï¼ˆé¿å…è¦†ç›–ï¼‰
            batch_delay: æ‰¹æ¬¡ä¹‹é—´çš„å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
            accumulate_data: æ˜¯å¦ç´¯ç§¯åˆå¹¶å†å²æ•°æ®ï¼ˆè§£å†³è¦†ç›–é—®é¢˜ï¼‰
        """
        try:
            # 1. è·å–Duneæ•°æ®
            df = self.get_dune_data(delay_seconds=batch_delay, preserve_batches=preserve_batches)

            # 2. å¤„ç†æ–°è·å–çš„é’±åŒ…æ•°æ®
            new_wallet_data = self.process_wallet_data(df)

            if not new_wallet_data:
                raise Exception("æœªæ‰¾åˆ°æœ‰æ•ˆçš„é’±åŒ…æ•°æ®")

            # 3. å¦‚æœå¯ç”¨ç´¯ç§¯æ¨¡å¼ï¼Œåˆå¹¶å†å²æ•°æ®
            if accumulate_data:
                logger.info("ğŸ”„ å¯ç”¨æ•°æ®ç´¯ç§¯æ¨¡å¼ï¼Œåˆå¹¶å†å²æ•°æ®...")
                existing_data = self.load_existing_wallet_data()
                wallet_data = self.merge_wallet_data(existing_data, new_wallet_data)
            else:
                logger.info("âš ï¸  æ•°æ®ç´¯ç§¯å·²å…³é—­ï¼Œåªä½¿ç”¨å½“å‰æ‰¹æ¬¡æ•°æ®")
                wallet_data = new_wallet_data

            # 4. æ ¹æ®é€‰æ‹©ä¿å­˜æ•°æ®
            if use_grouped_storage:
                self.save_optimized_data(wallet_data)
            else:
                self.create_simple_lookup_api_data(wallet_data)

            # 4. ä¿å­˜åŸå§‹å®Œæ•´æ•°æ®ï¼ˆå¤‡ä»½ï¼‰
            backup_file = os.path.join(self.data_dir, "full_wallet_data_backup.json")
            with open(backup_file, 'w', encoding='utf-8') as f:
                json.dump(wallet_data, f, indent=2, ensure_ascii=False)

            logger.info("æ•°æ®è·å–å’Œå­˜å‚¨å®Œæˆï¼")

            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            total_wallets = len(wallet_data)
            total_pairs = sum(len(pairs) for pairs in wallet_data.values())
            avg_pairs_per_wallet = total_pairs / total_wallets

            print(f"\n=== æ•°æ®ç»Ÿè®¡ ===")
            print(f"æ€»é’±åŒ…æ•°: {total_wallets:,}")
            print(f"æ€»äº¤æ˜“å¯¹æ•°: {total_pairs:,}")
            print(f"å¹³å‡æ¯é’±åŒ…äº¤æ˜“å¯¹æ•°: {avg_pairs_per_wallet:.2f}")
            print(f"æ•°æ®å­˜å‚¨ç›®å½•: {self.data_dir}")
            print(f"åŒºå—é“¾: Solana")
            print(f"é¡¹ç›®: Meteora DLMM")

        except Exception as e:
            logger.error(f"æ•°æ®è·å–å¤±è´¥: {str(e)}")
            raise


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ Meteora ç›ˆåˆ©æŸ¥è¯¢å™¨ - æ•°æ®è·å–å·¥å…·")
    print("æ”¯æŒåˆ†æ‰¹æ¬¡æ‹‰å–å’Œåˆå¹¶Duneæ•°æ®\n")

    # é»˜è®¤é…ç½® - å¯ä»¥ç›´æ¥åœ¨è¿™é‡Œä¿®æ”¹
    DEFAULT_QUERY_IDS = [5556654]  # åœ¨è¿™é‡Œæ·»åŠ ä½ çš„æŸ¥è¯¢IDåˆ—è¡¨
    DEFAULT_BATCH_DELAY = 1.0  # æ‰¹æ¬¡é—´å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰

    # å¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–é»˜è®¤é…ç½®
    query_ids_env = os.getenv('DUNE_QUERY_IDS')
    if query_ids_env:
        try:
            # æ”¯æŒé€—å·åˆ†éš”çš„æŸ¥è¯¢ID: "5556654,5556655,5556656"
            query_ids = [int(id.strip()) for id in query_ids_env.split(',')]
            print(f"âœ… ä»ç¯å¢ƒå˜é‡è¯»å–æŸ¥è¯¢ID: {query_ids}")
        except ValueError:
            print(f"âš ï¸  ç¯å¢ƒå˜é‡DUNE_QUERY_IDSæ ¼å¼é”™è¯¯ï¼Œä½¿ç”¨é»˜è®¤å€¼: {DEFAULT_QUERY_IDS}")
            query_ids = DEFAULT_QUERY_IDS
    else:
        query_ids = DEFAULT_QUERY_IDS
        print(f"âœ… ä½¿ç”¨é»˜è®¤æŸ¥è¯¢ID: {query_ids}")

    # æ‰¹æ¬¡å»¶è¿Ÿé…ç½®
    batch_delay_env = os.getenv('BATCH_DELAY')
    if batch_delay_env:
        try:
            batch_delay = float(batch_delay_env)
            print(f"âœ… ä»ç¯å¢ƒå˜é‡è¯»å–æ‰¹æ¬¡å»¶è¿Ÿ: {batch_delay} ç§’")
        except ValueError:
            print(f"âš ï¸  ç¯å¢ƒå˜é‡BATCH_DELAYæ ¼å¼é”™è¯¯ï¼Œä½¿ç”¨é»˜è®¤å€¼: {DEFAULT_BATCH_DELAY} ç§’")
            batch_delay = DEFAULT_BATCH_DELAY
    else:
        batch_delay = DEFAULT_BATCH_DELAY
        print(f"âœ… ä½¿ç”¨é»˜è®¤æ‰¹æ¬¡å»¶è¿Ÿ: {batch_delay} ç§’")

    try:
        # åˆ›å»ºæ•°æ®è·å–å™¨
        fetcher = MeteoraDataFetcher(query_ids)

        print(f"\nğŸ“‹ é…ç½®æ‘˜è¦:")
        print(f"   æŸ¥è¯¢IDåˆ—è¡¨: {query_ids}")
        print(f"   æ‰¹æ¬¡æ•°é‡: {len(query_ids)}")
        print(f"   æ‰¹æ¬¡å»¶è¿Ÿ: {batch_delay} ç§’")

        print("\n" + "=" * 60)
        print("å¼€å§‹æ•°æ®è·å–æµç¨‹...")
        print("=" * 60)

        # è¿è¡Œæ•°æ®è·å–ï¼Œä½¿ç”¨åˆ†ç»„å­˜å‚¨
        fetcher.run_data_fetch(
            use_grouped_storage=True
        )

        print("\nğŸ‰ æ‰€æœ‰æ“ä½œå®Œæˆï¼")
        print("\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
        print(f"   - æ‰¹æ¬¡æ•°æ®: {fetcher.batch_data_dir}/")
        print(f"   - åˆå¹¶æ•°æ®: {fetcher.data_dir}/merged_dune_data.*")
        print(f"   - é’±åŒ…æ•°æ®: {fetcher.data_dir}/wallet_group_*.json")
        print(f"   - æ‘˜è¦ä¿¡æ¯: {fetcher.data_dir}/merge_summary.json")

        print("\nâœ… ç°åœ¨å¯ä»¥ä½¿ç”¨å‰ç«¯é¡µé¢è¿›è¡ŒSolanaé’±åŒ…ç›ˆåˆ©æŸ¥è¯¢äº†ã€‚")
        print("âš ï¸  æ³¨æ„ï¼šè¯·ä½¿ç”¨Solanaé’±åŒ…åœ°å€è¿›è¡ŒæŸ¥è¯¢")

    except ValueError as e:
        print(f"âŒ é…ç½®é”™è¯¯: {str(e)}")
        print("è¯·ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º .env æ–‡ä»¶ï¼Œå¹¶è®¾ç½® DUNE_API_KEY=your_api_key_here")
    except KeyboardInterrupt:
        print(f"\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {str(e)}")


# ç¼–ç¨‹å¼ä½¿ç”¨ç¤ºä¾‹
def custom_batch_example():
    """è‡ªå®šä¹‰æ‰¹æ¬¡ç¤ºä¾‹"""
    print("ğŸ“š è‡ªå®šä¹‰æ‰¹æ¬¡ä½¿ç”¨ç¤ºä¾‹:")

    query_ids = [5556654]
    fetcher = MeteoraDataFetcher(query_ids)

    # æ–¹å¼2: åŠ¨æ€æ·»åŠ æŸ¥è¯¢ID
    # fetcher.add_query_id(5556654)

    # æ–¹å¼3: è‡ªå®šä¹‰å»¶è¿Ÿæ—¶é—´
    # fetcher.run_data_fetch(batch_delay=3.0)

    return fetcher


if __name__ == "__main__":
    main()
